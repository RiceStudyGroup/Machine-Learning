## Why is ReLU better and more often used than Sigmoid in Neural Networks?

sigmoid will make some neurons dead.

the derivative of ReLU is easier to calculate.
